{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"HPE Storage Array Exporter for Prometheus \u00b6 The HPE Storage Array Exporter for Prometheus provides observability for an HPE storage system via the Prometheus metric format. Synopsis \u00b6 This documentation shows how to deploy the exporter, the metrics it provides, and how to integrate it with Prometheus . Compatibility \u00b6 HPE Storage Array Exporter for Prometheus is compatible with these storage arrays: HPE Alletra 9000 HPE Alletra 6000 HPE Nimble Storage HPE Primera HPE 3PAR See the compatibility chart for specific versions and releases. Support \u00b6 HPE Storage Array Exporter for Prometheus is supported by HPE when used with HPE storage arrays on valid support contracts. Please send an email to ps-containervision-core@hpe.com to get started with any issue that requires assistance. Engage your HPE representative for other means to contact HPE Storage directly.","title":"HPE Storage Array Exporter for Prometheus"},{"location":"index.html#hpe_storage_array_exporter_for_prometheus","text":"The HPE Storage Array Exporter for Prometheus provides observability for an HPE storage system via the Prometheus metric format.","title":"HPE Storage Array Exporter for Prometheus"},{"location":"index.html#synopsis","text":"This documentation shows how to deploy the exporter, the metrics it provides, and how to integrate it with Prometheus .","title":"Synopsis"},{"location":"index.html#compatibility","text":"HPE Storage Array Exporter for Prometheus is compatible with these storage arrays: HPE Alletra 9000 HPE Alletra 6000 HPE Nimble Storage HPE Primera HPE 3PAR See the compatibility chart for specific versions and releases.","title":"Compatibility"},{"location":"index.html#support","text":"HPE Storage Array Exporter for Prometheus is supported by HPE when used with HPE storage arrays on valid support contracts. Please send an email to ps-containervision-core@hpe.com to get started with any issue that requires assistance. Engage your HPE representative for other means to contact HPE Storage directly.","title":"Support"},{"location":"deployment/index.html","text":"An HPE Storage Array Exporter for Prometheus deployment provides Prometheus metrics for a single storage system. A deployment can use an executable file or a container image. Configuration Command Options Using an Executable File Command Example Using a Container Image Docker Example Using a Kubernetes Deployment Configuration \u00b6 The address and credentials of the target storage system must be specified in a configuration file, using the format shown in this example storage-system.yaml file. address: 10.10.10.1 username: exampleuser password: examplepassword The address value is either a resolvable hostname or IP address of the management interface on the storage system. The username value identifies a storage system user with privileges described below. Storage System User Type Minimal Role HPE Alletra 9000, Primera, 3PAR System User Browse HPE Alletra 6000, Nimble Storage System User Guest HPE Alletra 6000, Nimble Storage 1 Tenant N/A 1 = NimbleOS 6.0 and above only. Command Options \u00b6 Option Default Description --accept-eula false Confirms your acceptance of the HPE license restrictions --log.path None A file location to write log messages, in addition to stdout --metrics.disable-introspection false Excludes metrics about the metrics provider itself, with prefixes such as promhttp , process , and go --telemetry.addr :8080 The host:port address at which to provide metrics --telemetry.path /metrics The endpoint path at which to provide metrics Using an Executable File \u00b6 Linux and Windows executable files are provided through GitHub releases . When an executable file has been copied to your server, it can be invoked with this command syntax: hpe-array-exporter [OPTION]... CONFIG-PATH Available OPTIONs are described in the Command Options section. CONFIG-PATH is the location of the storage system configuration file. Command Example \u00b6 ./hpe-array-exporter --log.path=/var/log/hpe-array-exporter.log /etc/config/storage-system.yaml Important Include the --accept-eula option or set the environment variable ACCEPT_HPE_STANDARD_EULA=yes to confirm your acceptance of the HPE license restrictions . Using a Container Image \u00b6 A container image is hosted at quay.io/hpestorage/array-exporter:v1.0.0 , with v1.0.0 replaced by the desired release version. When deploying the array exporter as a container, the configuration file must be mounted as a volume. Available options, including the --log.path used in the example below, are described in the Command Options section. Docker Example \u00b6 In this example, the configuration file at /tmp/storage-system.yaml is bound to the container's /etc/config/ directory as a volume using Docker's -v command option. The configuration file location inside the container is then given as a command argument. In addition, the -p option is used to map the container's port 8080 to port 9090 on the Docker host. docker run -it --name hpe-array-exporter -p 9090:8080 \\ -v /tmp/storage-system.yaml:/etc/config/storage-system.yaml \\ quay.io/hpestorage/array-exporter:v1.0.0 \\ --log.path /var/log/hpe-array-exporter.log \\ /etc/config/storage-system.yaml Important Include the --accept-eula option or set the environment variable ACCEPT_HPE_STANDARD_EULA=yes to confirm your acceptance of the HPE license restrictions . Consult the Docker command line documentation for more information on running containers using Docker. Using a Kubernetes Deployment \u00b6 Kubernetes deployment facilities are hosted in the co-deployments repository , including a Helm chart (via Artifact Hub) and sample YAML files .","title":"Deployment"},{"location":"deployment/index.html#configuration","text":"The address and credentials of the target storage system must be specified in a configuration file, using the format shown in this example storage-system.yaml file. address: 10.10.10.1 username: exampleuser password: examplepassword The address value is either a resolvable hostname or IP address of the management interface on the storage system. The username value identifies a storage system user with privileges described below. Storage System User Type Minimal Role HPE Alletra 9000, Primera, 3PAR System User Browse HPE Alletra 6000, Nimble Storage System User Guest HPE Alletra 6000, Nimble Storage 1 Tenant N/A 1 = NimbleOS 6.0 and above only.","title":"Configuration"},{"location":"deployment/index.html#command_options","text":"Option Default Description --accept-eula false Confirms your acceptance of the HPE license restrictions --log.path None A file location to write log messages, in addition to stdout --metrics.disable-introspection false Excludes metrics about the metrics provider itself, with prefixes such as promhttp , process , and go --telemetry.addr :8080 The host:port address at which to provide metrics --telemetry.path /metrics The endpoint path at which to provide metrics","title":"Command Options"},{"location":"deployment/index.html#using_an_executable_file","text":"Linux and Windows executable files are provided through GitHub releases . When an executable file has been copied to your server, it can be invoked with this command syntax: hpe-array-exporter [OPTION]... CONFIG-PATH Available OPTIONs are described in the Command Options section. CONFIG-PATH is the location of the storage system configuration file.","title":"Using an Executable File"},{"location":"deployment/index.html#command_example","text":"./hpe-array-exporter --log.path=/var/log/hpe-array-exporter.log /etc/config/storage-system.yaml Important Include the --accept-eula option or set the environment variable ACCEPT_HPE_STANDARD_EULA=yes to confirm your acceptance of the HPE license restrictions .","title":"Command Example"},{"location":"deployment/index.html#using_a_container_image","text":"A container image is hosted at quay.io/hpestorage/array-exporter:v1.0.0 , with v1.0.0 replaced by the desired release version. When deploying the array exporter as a container, the configuration file must be mounted as a volume. Available options, including the --log.path used in the example below, are described in the Command Options section.","title":"Using a Container Image"},{"location":"deployment/index.html#docker_example","text":"In this example, the configuration file at /tmp/storage-system.yaml is bound to the container's /etc/config/ directory as a volume using Docker's -v command option. The configuration file location inside the container is then given as a command argument. In addition, the -p option is used to map the container's port 8080 to port 9090 on the Docker host. docker run -it --name hpe-array-exporter -p 9090:8080 \\ -v /tmp/storage-system.yaml:/etc/config/storage-system.yaml \\ quay.io/hpestorage/array-exporter:v1.0.0 \\ --log.path /var/log/hpe-array-exporter.log \\ /etc/config/storage-system.yaml Important Include the --accept-eula option or set the environment variable ACCEPT_HPE_STANDARD_EULA=yes to confirm your acceptance of the HPE license restrictions . Consult the Docker command line documentation for more information on running containers using Docker.","title":"Docker Example"},{"location":"deployment/index.html#using_a_kubernetes_deployment","text":"Kubernetes deployment facilities are hosted in the co-deployments repository , including a Helm chart (via Artifact Hub) and sample YAML files .","title":"Using a Kubernetes Deployment"},{"location":"grafana/index.html","text":"Example Grafana dashboards, provided as is, are hosted on grafana.com .","title":"Grafana Dashboards"},{"location":"integration/index.html","text":"Overview \u00b6 When an HPE Storage Array Exporter for Prometheus has been deployed, a Prometheus instance can be configured to collect metrics from it. The configuration requirements depend on multiple factors, including the ways Prometheus and the exporter are deployed and the desired monitoring practices. The examples provided here are intended to facilitate setup of the exporter as a scrape target in select scenarios. Refer to Prometheus documentation for more complete guidance. Overview Prometheus Standalone Using a Prometheus Configuration File Prometheus running on Kubernetes Using a Kubernetes Service with Prometheus Helm Annotations Using a Kubernetes Service Monitor ServiceMonitor Example Important If the intention is to use the HPE provided Grafana dashboards, a label named \"array\" in scrape targets with the storage system designation (i.e. a hostname) is mandatory for the dashboards to work. Prometheus Standalone \u00b6 It's most common to have scrape targets statically defined in a Prometheus configuration file when running Prometheus as a standalone application. Using a Prometheus Configuration File \u00b6 When running Prometheus as either a standalone executable or a container, a configuration file (conventionally named prometheus.yml) is used. In it, a scrape job can be added for the exporter with configuration content similar to the following example. scrape_configs: - job_name: 'hpe-array-exporter' # A scrape interval of 30 seconds or more is recommended scrape_interval: 1m static_configs: # Set the target to the host address and port at which the exporter serves its metrics - targets: ['localhost:8080'] labels: # Specify any labels to be added to the resulting metrics Note It may be desirable to include a label to identify the storage system from which the metrics are collected. Prometheus running on Kubernetes \u00b6 When running Prometheus on Kubernetes, scrape targets may be dynamically discovered. Different methods are used when Prometheus is deployed via a Helm chart or an Operator. Using a Kubernetes Service with Prometheus Helm Annotations \u00b6 This example applies when the Prometheus Helm chart and the exporter are deployed in the same Kubernetes cluster. The Prometheus Helm chart uses a configuration file like the one described in the preceding section. Its default configuration enables Pods and Services to be identified as scrape targets using annotations such as prometheus.io/scrape: \"true\" and prometheus.io/scrape-slow: \"true\" . Annotations like these can be added to the exporter Service when created with either the sample YAML files or the Helm chart . Using a Kubernetes Service Monitor \u00b6 This example applies when the Prometheus Operator and the exporter are deployed in the same Kubernetes cluster. The Prometheus Operator and the Helm Kube Prometheus Stack can discover scrape targets via Service and ServiceMonitor objects. A Kubernetes Service is created along with a Deployment when using either the sample YAML files or the Helm chart . By default it is a ClusterIP Service for access from within the cluster. A NodePort Service can be configured instead to provide access from outside the cluster, for example in conjunction with a Prometheus configuration file as described above. A ServiceMonitor enables a Prometheus custom resource (defined by the Prometheus Operator) to discover the exporter Service as a scrape target. Whether the Prometheus instance is created automatically (such as by a Helm chart installation) or manually, its serviceMonitorNamespaceSelector and serviceMonitorSelector fields control which ServiceMonitor objects it discovers. The ServiceMonitor's namespaceSelector and selector fields in turn control how an exporter Service is identified. In the following example, the selector identifies the exporter Service based on a matching app label. ServiceMonitor Example \u00b6 kind: ServiceMonitor apiVersion: monitoring.coreos.com/v1 metadata: name: hpe-array-exporter-servicemonitor namespace: hpe-storage labels: # A selector for this label is used by a Prometheus Operator, # installed via OLM k8s-app: prometheus # A \"release\" label selector is used by the Helm Kube Prometheus Stack, # with the value as the release specified on chart installation release: prometheus spec: # Match the namespace of the target Array Exporter service, # or omit the namespaceSelector namespaceSelector: matchNames: - hpe-storage selector: matchLabels: app: hpe-array-exporter endpoints: - port: http-metrics scheme: http interval: 1m # Corresponding labels on the Array Exporter service are added to # the scraped metrics; customize as desired targetLabels: - array Note It may be desirable for metrics from the exporter to include a label identifying the storage system from which they are collected. In this example, the targetLabels configuration refers to an array label that must have been included in the Service object. A ServiceMonitor example can also be found in the sample YAML files .","title":"Prometheus Integration"},{"location":"integration/index.html#overview","text":"When an HPE Storage Array Exporter for Prometheus has been deployed, a Prometheus instance can be configured to collect metrics from it. The configuration requirements depend on multiple factors, including the ways Prometheus and the exporter are deployed and the desired monitoring practices. The examples provided here are intended to facilitate setup of the exporter as a scrape target in select scenarios. Refer to Prometheus documentation for more complete guidance. Overview Prometheus Standalone Using a Prometheus Configuration File Prometheus running on Kubernetes Using a Kubernetes Service with Prometheus Helm Annotations Using a Kubernetes Service Monitor ServiceMonitor Example Important If the intention is to use the HPE provided Grafana dashboards, a label named \"array\" in scrape targets with the storage system designation (i.e. a hostname) is mandatory for the dashboards to work.","title":"Overview"},{"location":"integration/index.html#prometheus_standalone","text":"It's most common to have scrape targets statically defined in a Prometheus configuration file when running Prometheus as a standalone application.","title":"Prometheus Standalone"},{"location":"integration/index.html#using_a_prometheus_configuration_file","text":"When running Prometheus as either a standalone executable or a container, a configuration file (conventionally named prometheus.yml) is used. In it, a scrape job can be added for the exporter with configuration content similar to the following example. scrape_configs: - job_name: 'hpe-array-exporter' # A scrape interval of 30 seconds or more is recommended scrape_interval: 1m static_configs: # Set the target to the host address and port at which the exporter serves its metrics - targets: ['localhost:8080'] labels: # Specify any labels to be added to the resulting metrics Note It may be desirable to include a label to identify the storage system from which the metrics are collected.","title":"Using a Prometheus Configuration File"},{"location":"integration/index.html#prometheus_running_on_kubernetes","text":"When running Prometheus on Kubernetes, scrape targets may be dynamically discovered. Different methods are used when Prometheus is deployed via a Helm chart or an Operator.","title":"Prometheus running on Kubernetes"},{"location":"integration/index.html#using_a_kubernetes_service_with_prometheus_helm_annotations","text":"This example applies when the Prometheus Helm chart and the exporter are deployed in the same Kubernetes cluster. The Prometheus Helm chart uses a configuration file like the one described in the preceding section. Its default configuration enables Pods and Services to be identified as scrape targets using annotations such as prometheus.io/scrape: \"true\" and prometheus.io/scrape-slow: \"true\" . Annotations like these can be added to the exporter Service when created with either the sample YAML files or the Helm chart .","title":"Using a Kubernetes Service with Prometheus Helm Annotations"},{"location":"integration/index.html#using_a_kubernetes_service_monitor","text":"This example applies when the Prometheus Operator and the exporter are deployed in the same Kubernetes cluster. The Prometheus Operator and the Helm Kube Prometheus Stack can discover scrape targets via Service and ServiceMonitor objects. A Kubernetes Service is created along with a Deployment when using either the sample YAML files or the Helm chart . By default it is a ClusterIP Service for access from within the cluster. A NodePort Service can be configured instead to provide access from outside the cluster, for example in conjunction with a Prometheus configuration file as described above. A ServiceMonitor enables a Prometheus custom resource (defined by the Prometheus Operator) to discover the exporter Service as a scrape target. Whether the Prometheus instance is created automatically (such as by a Helm chart installation) or manually, its serviceMonitorNamespaceSelector and serviceMonitorSelector fields control which ServiceMonitor objects it discovers. The ServiceMonitor's namespaceSelector and selector fields in turn control how an exporter Service is identified. In the following example, the selector identifies the exporter Service based on a matching app label.","title":"Using a Kubernetes Service Monitor"},{"location":"integration/index.html#servicemonitor_example","text":"kind: ServiceMonitor apiVersion: monitoring.coreos.com/v1 metadata: name: hpe-array-exporter-servicemonitor namespace: hpe-storage labels: # A selector for this label is used by a Prometheus Operator, # installed via OLM k8s-app: prometheus # A \"release\" label selector is used by the Helm Kube Prometheus Stack, # with the value as the release specified on chart installation release: prometheus spec: # Match the namespace of the target Array Exporter service, # or omit the namespaceSelector namespaceSelector: matchNames: - hpe-storage selector: matchLabels: app: hpe-array-exporter endpoints: - port: http-metrics scheme: http interval: 1m # Corresponding labels on the Array Exporter service are added to # the scraped metrics; customize as desired targetLabels: - array Note It may be desirable for metrics from the exporter to include a label identifying the storage system from which they are collected. In this example, the targetLabels configuration refers to an array label that must have been included in the Service object. A ServiceMonitor example can also be found in the sample YAML files .","title":"ServiceMonitor Example"},{"location":"legal/eula/index.html","text":"This software is provided according to HPE license restrictions . The deployment documentation describes how to indicate your acceptance of these terms.","title":"End User License Agreement"},{"location":"legal/support/index.html","text":"The HPE Storage Array Exporter for Prometheus is supported by HPE when used with HPE storage arrays on valid support contracts. Send email to ps-containervision-core@hpe.com to get started with any issue that requires assistance. Engage your HPE representative for other means to contact HPE Storage directly.","title":"Support"},{"location":"metrics/index.html","text":"Overview \u00b6 The metrics provided by the HPE Storage Array Exporter for Prometheus vary according to the storage system to which it is connected. Overview HPE Alletra 9000 and Primera Common Provisioning Group (CPG) Space Volume Space Volume Performance HPE Alletra 6000 and Nimble Storage Pool Space Volume Space Volume Performance HPE Alletra 9000 and Primera \u00b6 The metrics provided for HPE Alletra 9000 and Primera storage systems (3PAR is also supported) are equivalent, with the metric names reflecting the storage system type. Common Provisioning Group (CPG) Space \u00b6 Metric Type Description hpealletra9000_cpg_capacity_bytes hpeprimera_cpg_capacity_bytes hpe3par_cpg_capacity_bytes Gauge Total capacity allocated to a CPG hpealletra9000_cpg_used_bytes hpeprimera_cpg_used_bytes hpe3par_cpg_used_bytes Gauge CPG capacity in use hpealletra9000_cpg_available_bytes hpeprimera_cpg_available_bytes hpe3par_cpg_available_bytes Gauge CPG capacity available hpealletra9000_cpg_volume_used_bytes hpeprimera_cpg_volume_used_bytes hpe3par_cpg_volume_used_bytes Gauge CPG capacity reserved for volumes hpealletra9000_cpg_snapshot_used_bytes hpeprimera_cpg_snapshot_used_bytes hpe3par_cpg_snapshot_used_bytes Gauge CPG capacity reserved for snapshots Each of these metrics includes the following label. Label Description cpg CPG name Volume Space \u00b6 Metric Type Labels Description hpealletra9000_volume_size_bytes hpeprimera_volume_size_bytes hpe3par_volume_size_bytes Gauge cpg, provisioning, volume Volume data capacity hpealletra9000_volume_used_bytes hpeprimera_volume_used_bytes hpe3par_volume_used_bytes Gauge cpg, provisioning, volume Space used for volume data, including reserved space for a fully-provisioned volume hpealletra9000_volume_snapshot_used_bytes hpeprimera_volume_snapshot_used_bytes hpe3par_volume_snapshot_used_bytes Gauge cpg, provisioning, snap_cpg, volume Space used for volume snapshots The labels have the following meanings. Label Description cpg Name of the CPG containing the volume provisioning Indicates whether storage space is reserved upon volume creation (\"thick\") or as needed (\"thin\") snap_cpg Name of the CPG containing the volume snapshots volume Volume name Volume Performance \u00b6 Metric Type Description hpealletra9000_volume_reads_per_second_avg5m hpeprimera_volume_reads_per_second_avg5m hpep3par_volume_reads_per_second_avg5m Gauge Read operations performed on the VLUNs for a volume per second, averaged over a fixed five-minute interval (read IOPS) hpealletra9000_volume_writes_per_second_avg5m hpeprimera_volume_writes_per_second_avg5m hpe3par_volume_writes_per_second_avg5m Gauge Write operations performed on the VLUNs for a volume per second, averaged over a fixed five-minute interval (write IOPS) hpealletra9000_volume_read_bytes_per_second_avg5m hpeprimera_volume_read_bytes_per_second_avg5m hpe3par_volume_read_bytes_per_second_avg5m Gauge Bytes read from the VLUNs for a volume per second, averaged over a fixed five-minute interval (read throughput) hpealletra9000_volume_write_bytes_per_second_avg5m hpeprimera_volume_write_bytes_per_second_avg5m hpe3par_volume_write_bytes_per_second_avg5m Gauge Bytes written to the VLUNs for a volume per second, averaged over a fixed five-minute interval (write throughput) hpealletra9000_volume_seconds_per_read_avg5m hpeprimera_volume_seconds_per_read_avg5m hpe3par_volume_seconds_per_read_avg5m Gauge Seconds elapsed during a single read from the VLUNs for a volume, averaged over a fixed five-minute interval (read latency) hpealletra9000_volume_seconds_per_write_avg5m hpeprimera_volume_seconds_per_write_avg5m hpe3par_volume_seconds_per_write_avg5m Gauge Seconds elapsed during a single write to the VLUNs for a volume, averaged over a fixed five-minute interval (write latency) Each of these metrics includes the following labels. Label Description cpg Name of the CPG containing the volume volume Volume name HPE Alletra 6000 and Nimble \u00b6 The metrics provided for HPE Alletra 6000 and Nimble Storage systems are equivalent, with the metric names reflecting the storage system type. Storage Pool Space \u00b6 Metric Type Description hpealletra6000_pool_capacity_bytes hpenimble_pool_capacity_bytes Gauge Total bytes that can be written into a storage pool hpealletra6000_pool_used_bytes hpenimble_pool_used_bytes Gauge Storage pool capacity in use hpealletra6000_pool_available_bytes hpenimble_pool_available_bytes Gauge Storage pool capacity available to provision volumes hpealletra6000_pool_unused_reserve_bytes hpenimble_pool_unused_reserve_bytes Gauge Storage pool capacity that is reserved for existing volumes but is not in use hpealletra6000_pool_volume_used_bytes hpenimble_pool_volume_used_bytes Gauge Total logical size of all volumes in a storage pool hpealletra6000_pool_snapshot_used_bytes hpenimble_pool_snapshot_used_bytes Gauge Total logical size of all snapshots in a storage pool hpealletra6000_pool_data_reduction_bytes hpenimble_pool_data_reduction_bytes Gauge Storage pool capacity that would be in use if not for storage system optimizations, including compression, deduplication, and clones, but excluding thin provisioning Each of these metrics includes the following label. Label Description pool Storage pool name Volume Space \u00b6 Metric Type Description hpealletra6000_volume_size_bytes hpenimble_volume_size_bytes Gauge Volume data capacity hpealletra6000_volume_used_bytes hpenimble_volume_used_bytes Gauge Space used for volume data hpealletra6000_volume_snapshot_used_bytes hpenimble_volume_snapshot_used_bytes Gauge Space used for volume snapshots Each of these metrics includes the following labels. Label Description pool Name of the storage pool containing the volume provisioning Indicates whether storage space is reserved upon volume creation (\"thick\") or as needed (\"thin\") volume Volume ID volume_name Volume name Volume Performance \u00b6 Metric Type Description hpealletra6000_volume_reads_per_second_avg5m hpenimble_volume_reads_per_second_avg5m Gauge Read operations performed on a volume per second, averaged over the preceding five minutes (read IOPS) hpealletra6000_volume_writes_per_second_avg5m hpenimble_volume_writes_per_second_avg5m Gauge Write operations performed on a volume per second, averaged over the preceding five minutes (write IOPS) hpealletra6000_volume_read_bytes_per_second_avg5m hpenimble_volume_read_bytes_per_second_avg5m Gauge Bytes read from a volume per second, averaged over the preceding five minutes (read throughput) hpealletra6000_volume_write_bytes_per_second_avg5m hpenimble_volume_write_bytes_per_second_avg5m Gauge Bytes written to a volume per second, averaged over the preceding five minutes (write throughput) hpealletra6000_volume_seconds_per_read_avg5m hpenimble_volume_seconds_per_read_avg5m Gauge Seconds elapsed during a single read from a volume, averaged over the preceding five minutes (read latency) hpealletra6000_volume_seconds_per_write_avg5m hpenimble_volume_seconds_per_write_avg5m Gauge Seconds elapsed during a single write to a volume, averaged over the preceding five minutes (write latency) Each of these metrics includes the following labels. Label Description pool Name of the storage pool containing the volume volume Volume ID volume_name Volume name","title":"Metrics"},{"location":"metrics/index.html#overview","text":"The metrics provided by the HPE Storage Array Exporter for Prometheus vary according to the storage system to which it is connected. Overview HPE Alletra 9000 and Primera Common Provisioning Group (CPG) Space Volume Space Volume Performance HPE Alletra 6000 and Nimble Storage Pool Space Volume Space Volume Performance","title":"Overview"},{"location":"metrics/index.html#hpe_alletra_9000_and_primera","text":"The metrics provided for HPE Alletra 9000 and Primera storage systems (3PAR is also supported) are equivalent, with the metric names reflecting the storage system type.","title":"HPE Alletra 9000 and Primera"},{"location":"metrics/index.html#common_provisioning_group_cpg_space","text":"Metric Type Description hpealletra9000_cpg_capacity_bytes hpeprimera_cpg_capacity_bytes hpe3par_cpg_capacity_bytes Gauge Total capacity allocated to a CPG hpealletra9000_cpg_used_bytes hpeprimera_cpg_used_bytes hpe3par_cpg_used_bytes Gauge CPG capacity in use hpealletra9000_cpg_available_bytes hpeprimera_cpg_available_bytes hpe3par_cpg_available_bytes Gauge CPG capacity available hpealletra9000_cpg_volume_used_bytes hpeprimera_cpg_volume_used_bytes hpe3par_cpg_volume_used_bytes Gauge CPG capacity reserved for volumes hpealletra9000_cpg_snapshot_used_bytes hpeprimera_cpg_snapshot_used_bytes hpe3par_cpg_snapshot_used_bytes Gauge CPG capacity reserved for snapshots Each of these metrics includes the following label. Label Description cpg CPG name","title":"Common Provisioning Group (CPG) Space"},{"location":"metrics/index.html#volume_space","text":"Metric Type Labels Description hpealletra9000_volume_size_bytes hpeprimera_volume_size_bytes hpe3par_volume_size_bytes Gauge cpg, provisioning, volume Volume data capacity hpealletra9000_volume_used_bytes hpeprimera_volume_used_bytes hpe3par_volume_used_bytes Gauge cpg, provisioning, volume Space used for volume data, including reserved space for a fully-provisioned volume hpealletra9000_volume_snapshot_used_bytes hpeprimera_volume_snapshot_used_bytes hpe3par_volume_snapshot_used_bytes Gauge cpg, provisioning, snap_cpg, volume Space used for volume snapshots The labels have the following meanings. Label Description cpg Name of the CPG containing the volume provisioning Indicates whether storage space is reserved upon volume creation (\"thick\") or as needed (\"thin\") snap_cpg Name of the CPG containing the volume snapshots volume Volume name","title":"Volume Space"},{"location":"metrics/index.html#volume_performance","text":"Metric Type Description hpealletra9000_volume_reads_per_second_avg5m hpeprimera_volume_reads_per_second_avg5m hpep3par_volume_reads_per_second_avg5m Gauge Read operations performed on the VLUNs for a volume per second, averaged over a fixed five-minute interval (read IOPS) hpealletra9000_volume_writes_per_second_avg5m hpeprimera_volume_writes_per_second_avg5m hpe3par_volume_writes_per_second_avg5m Gauge Write operations performed on the VLUNs for a volume per second, averaged over a fixed five-minute interval (write IOPS) hpealletra9000_volume_read_bytes_per_second_avg5m hpeprimera_volume_read_bytes_per_second_avg5m hpe3par_volume_read_bytes_per_second_avg5m Gauge Bytes read from the VLUNs for a volume per second, averaged over a fixed five-minute interval (read throughput) hpealletra9000_volume_write_bytes_per_second_avg5m hpeprimera_volume_write_bytes_per_second_avg5m hpe3par_volume_write_bytes_per_second_avg5m Gauge Bytes written to the VLUNs for a volume per second, averaged over a fixed five-minute interval (write throughput) hpealletra9000_volume_seconds_per_read_avg5m hpeprimera_volume_seconds_per_read_avg5m hpe3par_volume_seconds_per_read_avg5m Gauge Seconds elapsed during a single read from the VLUNs for a volume, averaged over a fixed five-minute interval (read latency) hpealletra9000_volume_seconds_per_write_avg5m hpeprimera_volume_seconds_per_write_avg5m hpe3par_volume_seconds_per_write_avg5m Gauge Seconds elapsed during a single write to the VLUNs for a volume, averaged over a fixed five-minute interval (write latency) Each of these metrics includes the following labels. Label Description cpg Name of the CPG containing the volume volume Volume name","title":"Volume Performance"},{"location":"metrics/index.html#hpe_alletra_6000_and_nimble","text":"The metrics provided for HPE Alletra 6000 and Nimble Storage systems are equivalent, with the metric names reflecting the storage system type.","title":"HPE Alletra 6000 and Nimble"},{"location":"metrics/index.html#storage_pool_space","text":"Metric Type Description hpealletra6000_pool_capacity_bytes hpenimble_pool_capacity_bytes Gauge Total bytes that can be written into a storage pool hpealletra6000_pool_used_bytes hpenimble_pool_used_bytes Gauge Storage pool capacity in use hpealletra6000_pool_available_bytes hpenimble_pool_available_bytes Gauge Storage pool capacity available to provision volumes hpealletra6000_pool_unused_reserve_bytes hpenimble_pool_unused_reserve_bytes Gauge Storage pool capacity that is reserved for existing volumes but is not in use hpealletra6000_pool_volume_used_bytes hpenimble_pool_volume_used_bytes Gauge Total logical size of all volumes in a storage pool hpealletra6000_pool_snapshot_used_bytes hpenimble_pool_snapshot_used_bytes Gauge Total logical size of all snapshots in a storage pool hpealletra6000_pool_data_reduction_bytes hpenimble_pool_data_reduction_bytes Gauge Storage pool capacity that would be in use if not for storage system optimizations, including compression, deduplication, and clones, but excluding thin provisioning Each of these metrics includes the following label. Label Description pool Storage pool name","title":"Storage Pool Space"},{"location":"metrics/index.html#volume_space_1","text":"Metric Type Description hpealletra6000_volume_size_bytes hpenimble_volume_size_bytes Gauge Volume data capacity hpealletra6000_volume_used_bytes hpenimble_volume_used_bytes Gauge Space used for volume data hpealletra6000_volume_snapshot_used_bytes hpenimble_volume_snapshot_used_bytes Gauge Space used for volume snapshots Each of these metrics includes the following labels. Label Description pool Name of the storage pool containing the volume provisioning Indicates whether storage space is reserved upon volume creation (\"thick\") or as needed (\"thin\") volume Volume ID volume_name Volume name","title":"Volume Space"},{"location":"metrics/index.html#volume_performance_1","text":"Metric Type Description hpealletra6000_volume_reads_per_second_avg5m hpenimble_volume_reads_per_second_avg5m Gauge Read operations performed on a volume per second, averaged over the preceding five minutes (read IOPS) hpealletra6000_volume_writes_per_second_avg5m hpenimble_volume_writes_per_second_avg5m Gauge Write operations performed on a volume per second, averaged over the preceding five minutes (write IOPS) hpealletra6000_volume_read_bytes_per_second_avg5m hpenimble_volume_read_bytes_per_second_avg5m Gauge Bytes read from a volume per second, averaged over the preceding five minutes (read throughput) hpealletra6000_volume_write_bytes_per_second_avg5m hpenimble_volume_write_bytes_per_second_avg5m Gauge Bytes written to a volume per second, averaged over the preceding five minutes (write throughput) hpealletra6000_volume_seconds_per_read_avg5m hpenimble_volume_seconds_per_read_avg5m Gauge Seconds elapsed during a single read from a volume, averaged over the preceding five minutes (read latency) hpealletra6000_volume_seconds_per_write_avg5m hpenimble_volume_seconds_per_write_avg5m Gauge Seconds elapsed during a single write to a volume, averaged over the preceding five minutes (write latency) Each of these metrics includes the following labels. Label Description pool Name of the storage pool containing the volume volume Volume ID volume_name Volume name","title":"Volume Performance"},{"location":"releases/index.html","text":"Overview \u00b6 HPE Storage Array Exporter for Prometheus is a unified exporter that supports the HPE primary storage portfolio. The metrics provided differ by storage system type and release. Delivery Vehicles \u00b6 The exporter is delivered as: Standalone binaries for Linux and Windows through GitHub releases. Container images hosted on Quay, deployable via standalone Docker or via Kubernetes (with object definitions in YAML available in GitHub). Helm charts hosted at Artifact Hub. HPE Storage Array Exporter for Prometheus 1.0.0-beta \u00b6 Release highlights: Initial beta and early access Binaries and Release Notes 1.0.0-beta on GitHub Helm Chart 1.0.0-beta on Artifact Hub Platforms Alletra OS 9000 9.3.0 Alletra OS 6000 6.0.0 or later Nimble OS 5.0.10.x or later Primera OS 4.0.x, 4.1.x, 4.2.x, 4.3.x 3PAR OS 3.3.1, 3.3.2","title":"Releases"},{"location":"releases/index.html#overview","text":"HPE Storage Array Exporter for Prometheus is a unified exporter that supports the HPE primary storage portfolio. The metrics provided differ by storage system type and release.","title":"Overview"},{"location":"releases/index.html#delivery_vehicles","text":"The exporter is delivered as: Standalone binaries for Linux and Windows through GitHub releases. Container images hosted on Quay, deployable via standalone Docker or via Kubernetes (with object definitions in YAML available in GitHub). Helm charts hosted at Artifact Hub.","title":"Delivery Vehicles"},{"location":"releases/index.html#hpe_storage_array_exporter_for_prometheus_100-beta","text":"Release highlights: Initial beta and early access Binaries and Release Notes 1.0.0-beta on GitHub Helm Chart 1.0.0-beta on Artifact Hub Platforms Alletra OS 9000 9.3.0 Alletra OS 6000 6.0.0 or later Nimble OS 5.0.10.x or later Primera OS 4.0.x, 4.1.x, 4.2.x, 4.3.x 3PAR OS 3.3.1, 3.3.2","title":"HPE Storage Array Exporter for Prometheus 1.0.0-beta"}]}